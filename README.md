# ğŸ“˜ Chat with PDFs

ğŸš€ Features

Interact with multiple PDF documents using LLMs and Qdrant (vector database). Upload PDFs, process them into embeddings, and ask natural language questions â€” with citations and cross-document reasoning.

- Upload and query multiple PDFs in natural language
- Store embeddings in Qdrant (local, via Docker)
- Hybrid search (semantic + keyword)
- Citations with page numbers and sections
- Modular design: extend with summarizers, agents, or a different vector DB

---

ğŸ“‚ Project Structure

```
chat-with-pdfs/
â”‚â”€â”€ .env                        # Environment variables
â”‚â”€â”€ requirements.txt            # Base dependencies
â”‚â”€â”€ docker-compose.yml          # (optional) orchestrates backend + Qdrant
â”‚â”€â”€ README.md
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ Dockerfile              # Backend container
â”‚   â””â”€â”€ entrypoint.py           # Starts FastAPI + Gradio app
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/                # User-uploaded PDFs
â”‚   â””â”€â”€ processed/              # Extracted & chunked text
â”‚
â”œâ”€â”€ ingestion/                  # PDF â†’ text â†’ embeddings pipeline
â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â”œâ”€â”€ text_splitter.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â””â”€â”€ pipeline.py
â”‚
â”œâ”€â”€ retrieval/                  # Search & retrieval
â”‚   â”œâ”€â”€ retriever.py
â”‚   â””â”€â”€ hybrid_search.py
â”‚
â”œâ”€â”€ llm/                        # LLM integration
â”‚   â”œâ”€â”€ chat_model.py
â”‚   â””â”€â”€ prompts.py
â”‚
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ gradio_app.py
â”‚   â””â”€â”€ streamlit_app.py
â”‚
â””â”€â”€ qdrant/
    â””â”€â”€ storage/                # Persistent DB storage
```

---

âš™ï¸ Setup Instructions

1. Clone the repository

```bash
git clone https://github.com/yourusername/chat-with-pdfs.git
cd chat-with-pdfs
```

2. Start Qdrant (Vector DB)

Run Qdrant container:

```bash
docker run -d -p 6333:6333 \
  -v $(pwd)/qdrant/storage:/qdrant/storage \
  qdrant/qdrant
```

Qdrant Dashboard â†’ http://localhost:6333

Data persists in qdrant/storage/

3. Build the backend image

```bash
docker build -t chat-pdfs-backend -f backend/Dockerfile .
```

This will:

- Install dependencies from requirements.txt
- Copy project code into /app inside the container
- Set up Gradio server at port 7860

4. Run the backend

```bash
docker run -p 7860:7860 chat-pdfs-backend
```

Now access the app at:
ğŸ‘‰ http://localhost:7860

---

ğŸ“š Usage

**Process PDFs**

Place PDFs in `data/uploads/` then run:

```bash
python ingestion/pipeline.py
```

This will:

- Extract text
- Split into chunks
- Generate embeddings
- Store them in Qdrant

**Query PDFs**

- Via Gradio UI â†’ http://localhost:7860
- Or via Streamlit (optional):

```bash
python ui/streamlit_app.py
```

---

ğŸ›‘ Stopping

**Stop backend:**

```bash
docker ps   # find container id
docker stop <container_id>
```

**Stop Qdrant:**

```bash
docker stop $(docker ps -q --filter ancestor=qdrant/qdrant)
```

Closing Docker Desktop also stops all containers.

---

ğŸ›  Tech Stack

- LLM: HuggingFace / local models
- Vector DB: Qdrant (Dockerized)
- Backend: FastAPI + Gradio
- Embeddings: SentenceTransformers
- UI: Gradio + Streamlit
