# ğŸ“˜ Chat with PDFs

Chat with multiple PDF documents using LLMs + Weaviate (vector DB).  
This project lets you upload PDFs, process them into embeddings, and then ask natural language questions â€” with citations and cross-document reasoning.

---

## ğŸš€ Features

- Upload multiple PDFs and query them in natural language
- Store embeddings in Weaviate (local or cloud)
- Hybrid search (semantic + keyword)
- Citations with page numbers + sections
- Modular design â†’ easy to extend with agents, summarizers, or different vector DBs

---

## ğŸ“‚ Project Structure

```
chat-with-pdfs/
â”‚â”€â”€ app.py                  # Main entry point (UI or API)
â”‚â”€â”€ config.py               # Config (DB settings, API keys)
â”‚â”€â”€ requirements.txt        # Dependencies
â”‚â”€â”€ README.md               # Documentation
â”‚
â”œâ”€â”€ data/                   # Raw and processed files
â”‚   â”œâ”€â”€ uploads/            # User-uploaded PDFs
â”‚   â”œâ”€â”€ processed/          # Extracted text/chunks
â”‚
â”œâ”€â”€ database/               # Vector DB logic
â”‚   â”œâ”€â”€ db_manager.py
â”‚   â”œâ”€â”€ schema.py
â”‚
â”œâ”€â”€ ingestion/              # PDF â†’ text â†’ chunks â†’ embeddings
â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â”œâ”€â”€ text_splitter.py
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚
â”œâ”€â”€ retrieval/              # Query pipeline
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ hybrid_search.py
â”‚
â”œâ”€â”€ llm/                    # LLM logic
â”‚   â”œâ”€â”€ chat_model.py
â”‚   â”œâ”€â”€ prompts.py
â”‚
â”œâ”€â”€ ui/                     # Frontend
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â”œâ”€â”€ gradio_app.py
â”‚
â””â”€â”€ tests/                  # Unit tests
```

---

## âš™ï¸ Setup Instructions

### 1. Clone Repo & Install Requirements

```sh
git clone https://github.com/yourusername/chat-with-pdfs.git
cd chat-with-pdfs
pip install -r requirements.txt
```

### 2. Install & Run Weaviate with Docker

**Install Docker**

- Download Docker Desktop (Windows/Mac)
- Linux:
  ```sh
  sudo apt update && sudo apt install docker.io -y
  ```

**Run Weaviate Container**

```sh
docker run -d -p 8080:8080 \
    -v weaviate_data:/var/lib/weaviate \
    semitechnologies/weaviate \
    --host 0.0.0.0 \
    --port 8080 \
    --modules-text2vec-openai
```

Access: [http://localhost:8080/v1/graphql](http://localhost:8080/v1/graphql)  
This persists data in a Docker volume `weaviate_data`.

### 3. Configure Project

Edit `config.py`:

```python
DB_TYPE = "weaviate"
WEAVIATE_URL = "http://localhost:8080"
WEAVIATE_API_KEY = None  # Not needed for local
EMBEDDING_DIM = 1536     # Match your embedding model
```

### 4. Process PDFs

Put your PDFs into `data/uploads/` and run:

```sh
python ingestion/pipeline.py
```

This will:

- Extract text
- Split into chunks
- Generate embeddings
- Store in Weaviate

### 5. Query the PDFs

Run the UI (Streamlit example):

```sh
streamlit run ui/streamlit_app.py
```

Now you can upload/query PDFs interactively ğŸ‰

---

## ğŸ›  Tech Stack

- **LLM:** OpenAI GPT / Local LLaMA
- **Vector DB:** Weaviate (local Docker or cloud)
- **Frontend:** Streamlit / Gradio
- **Embeddings:** OpenAI or SentenceTransformers
