# ğŸ“˜ Chat with PDFs

Chat with multiple PDF documents using LLMs + Weaviate (vector DB).  
This project lets you upload PDFs, process them into embeddings, and then ask natural language questions â€” with citations and cross-document reasoning.

---

## ğŸš€ Features

<h1>ğŸ“˜ Chat with PDFs</h1>

Interact with multiple PDF documents using LLMs and Qdrant (vector database). Upload PDFs, process them into embeddings, and ask natural language questions â€” with citations and cross-document reasoning.

---

## ğŸš€ Features

- Upload and query multiple PDFs in natural language
- Store embeddings in Qdrant (local, via Docker)
- Hybrid search (semantic + keyword)
- Citations with page numbers and sections
- Modular design: easily extend with agents, summarizers, or different vector DBs

---

## ğŸ“‚ Project Structure

```
chat-with-pdfs/
â”‚â”€â”€ .env                        # Environment variables
â”‚â”€â”€ requirements.txt            # Base dependencies for backend
â”‚â”€â”€ docker-compose.yml          # Orchestrates backend, UI, DB
â”‚â”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/                # User-uploaded PDFs
â”‚   â””â”€â”€ processed/              # Extracted & chunked text
â”‚
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ db_manager.py           # Connects to Qdrant
â”‚   â””â”€â”€ schema.py               # Metadata schema (page, section, etc.)
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pdf_loader.py           # Extracts text from PDFs
â”‚   â”œâ”€â”€ text_splitter.py        # Splits into chunks
â”‚   â”œâ”€â”€ embeddings.py           # Generates embeddings (HuggingFace/OpenAI)
â”‚   â””â”€â”€ pipeline.py             # Orchestration: load â†’ split â†’ embed â†’ store
â”‚
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ retriever.py            # Retrieves relevant chunks
â”‚   â”œâ”€â”€ reranker.py             # (Optional) re-rank results
â”‚   â””â”€â”€ hybrid_search.py        # If using keyword+vector
â”‚
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chat_model.py           # Wrapper for GPT/LLaMA etc.
â”‚   â”œâ”€â”€ prompts.py              # Custom prompts (summaries, compare, explain)
â”‚   â””â”€â”€ agents.py               # Specialized agents (summarizer, comparer, reasoner)
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logging.py
â”‚   â”œâ”€â”€ pdf_utils.py
â”‚   â””â”€â”€ text_cleaning.py
â”‚
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â”œâ”€â”€ gradio_app.py
â”‚   â””â”€â”€ react_frontend/         # Optional full React UI
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ Dockerfile              # Docker container for backend
â”‚   â””â”€â”€ entrypoint.py           # Starts app or orchestrates services
â”‚
â””â”€â”€ qdrant/
    â””â”€â”€ storage/                # Persistent DB storage for Qdrant
```

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository & Install Requirements

```bash
git clone https://github.com/yourusername/chat-with-pdfs.git
cd chat-with-pdfs
pip install -r requirements.txt
```

### 2. Install & Run Qdrant with Docker

- **Install Docker**
  - Windows/Mac: Download Docker Desktop
  - Linux:
    ```bash
    sudo apt update && sudo apt install docker.io -y
    ```

- **Run Qdrant Container**
  ```bash
  docker run -d -p 6333:6333 \
    -v $(pwd)/qdrant/storage:/qdrant/storage \
    qdrant/qdrant
  ```
  - Access Qdrant at: http://localhost:6333
  - Data persists in `qdrant/storage/` volume

### 3. Configure the Project

Edit `.env` or `config.py` as needed:

```python
DB_TYPE = "qdrant"
QDRANT_URL = "http://localhost:6333"
EMBEDDING_DIM = 1536  # Match your embedding model
```

### 4. Process PDFs

Place your PDFs in `data/uploads/` and run:

```bash
python ingestion/pipeline.py
```

This will:
- Extract text
- Split into chunks
- Generate embeddings
- Store in Qdrant

### 5. Run the UI

- **Gradio:**
  ```bash
  python ui/gradio_app.py
  ```

Access the interface at:
- Gradio: http://localhost:7860
- Streamlit: http://localhost:8501

---

## ğŸ›  Tech Stack

- **LLM:** Local LLaMA / Hugging Face models
- **Vector DB:** Qdrant (via Docker)
- **Frontend:** Gradio 
- **Embeddings:** SentenceTransformers 
